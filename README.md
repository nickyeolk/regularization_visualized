# regularization_visualized
## A better visualization of comparisons between L1 (lasso) and L2 (ridge) regularization and why L1 regularization shrinks weights to 0.  
You often hear the saying "L1 regularization tends to shrink the coefficients of unimportant features to 0, but L2 does not" in all the good explanations of regularization as seen in [here](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) and [here](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261). Visual explanations usually consist of this very popular picture from Figure 3.11 from Elements of Statistical Learning by Hastie, Tibshirani, and Friedman,  
![visual explanation of regularization](https://i.stack.imgur.com/oVJDB.png)   
also seen here in Pattern Recognition and Machine Learning by Bishop ![another visual explanation of regularization](https://i.stack.imgur.com/ffUlW.png).  
I have found these diagrams unintuitive, and have thus made a simpler one that feels much easier to understand.
